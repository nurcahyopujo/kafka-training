{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python375jvsc74a57bd07d6993cb2f9ce9a59d5d7380609d9cb5192a9dedd2735a011418ad9e827eb538",
      "display_name": "Python 3.7.5 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "01 - Producer.ipynb",
      "provenance": []
    },
    "metadata": {
      "interpreter": {
        "hash": "7d6993cb2f9ce9a59d5d7380609d9cb5192a9dedd2735a011418ad9e827eb538"
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBwSa4Wz_3bG"
      },
      "source": [
        "# Kafka Producer\n",
        "\n",
        "---"
      ],
      "id": "KBwSa4Wz_3bG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQPrmtMR_3bL"
      },
      "source": [
        "<img src=\"https://learning.oreilly.com/library/view/kafka-the-definitive/9781492043072/assets/ktdg_0301.png\" alt=\"producer\" width=\"500\"/>\n",
        "---\n",
        "\n",
        "## Preparation\n",
        "The first step is to install the required libraries, in our case `kafka-python` (and `pandas` to support visualization)"
      ],
      "id": "MQPrmtMR_3bL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUr-y6pX_3bM",
        "outputId": "a855a364-a94b-45c2-8958-ac22a9e2dad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install kafka-python"
      ],
      "id": "vUr-y6pX_3bM",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "declare variable"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "username = \"pujo\"\n",
        "server_ip = \"34.87.150.250\"\n",
        "bootstrap_servers = f\"{server_ip}:9092,{server_ip}:9093,{server_ip}:9094\"\n",
        "schema_registry_url = \"http://34.87.150.250:8081\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdaeK4Te_3bM"
      },
      "source": [
        "---\n",
        "\n",
        "## Async and Sync \n",
        "\n",
        "Now we can create a Kafka Producer\n"
      ],
      "id": "XdaeK4Te_3bM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kafka import KafkaProducer\n",
        "\n",
        "topic_name = f\"{username}-topic1\"\n",
        "\n",
        "producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
        "\n",
        "for _ in range(10):\n",
        "    producer.send(topic_name, b'some_message_bytes')"
      ]
    },
    {
      "source": [
        "function `producer.send` is asynchronous by default"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(100):\n",
        "    producer.send(topic_name, b'some_message_bytes')\n",
        "\n",
        "producer.flush()"
      ]
    },
    {
      "source": [
        "you could change it to synchronous operation by using `get` function"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kafka.errors import KafkaError\n",
        "\n",
        "producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
        "# producer = KafkaProducer(bootstrap_servers=bootstrap_servers, max_request_size=1) # failed producer scenario\n",
        "\n",
        "future = producer.send(topic_name, b'synchronous message')\n",
        "\n",
        "# Block for 'synchronous' sends\n",
        "try:\n",
        "    record_metadata = future.get(timeout=10)\n",
        "\n",
        "    # Successful result returns assigned partition and offset\n",
        "    print (record_metadata.topic)\n",
        "    print (record_metadata.partition)\n",
        "    print (record_metadata.offset)\n",
        "\n",
        "except KafkaError:\n",
        "    # Decide what to do if produce request failed...\n",
        "    print(\"send data failed\")\n",
        "    pass\n"
      ]
    },
    {
      "source": [
        "handle exception on asynchronous send with callback"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
        "# producer = KafkaProducer(bootstrap_servers=bootstrap_servers, max_request_size=1) # failed producer scenario\n",
        "\n",
        "def on_send_success(record_metadata):\n",
        "    print(record_metadata.topic)\n",
        "    print(record_metadata.partition)\n",
        "    print(record_metadata.offset)\n",
        "\n",
        "def on_send_error(excp):\n",
        "    print(\"handle error\")\n",
        "    print(excp)\n",
        "    log.error('I am an errback', exc_info=excp)\n",
        "    # handle exception\n",
        "\n",
        "producer \\\n",
        "    .send(topic_name, b'mesage async') \\\n",
        "    .add_callback(on_send_success) \\\n",
        "    .add_errback(on_send_error)\n",
        "producer.flush()"
      ]
    },
    {
      "source": [
        "configure multiple retries"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "producer = KafkaProducer(bootstrap_servers=bootstrap_servers, retries=5)\n",
        "# producer = KafkaProducer(bootstrap_servers=bootstrap_servers, retries=5, max_request_size=1) # failed producer scenario\n",
        "\n",
        "def on_send_success(record_metadata):\n",
        "    print(record_metadata.topic)\n",
        "    print(record_metadata.partition)\n",
        "    print(record_metadata.offset)\n",
        "\n",
        "def on_send_error(excp):\n",
        "    print(\"handle error\")\n",
        "    print(excp)\n",
        "    log.error('I am an errback', exc_info=excp)\n",
        "    # handle exception\n",
        "\n",
        "producer \\\n",
        "    .send(topic_name, b'mesage async') \\\n",
        "    .add_callback(on_send_success) \\\n",
        "    .add_errback(on_send_error)\n",
        "producer.flush()"
      ]
    },
    {
      "source": [
        "---\n",
        "\n",
        "## Configuring Producer\n",
        "\n",
        "Kafka producer API https://kafka-python.readthedocs.io/en/master/apidoc/KafkaProducer.html"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "producer = KafkaProducer(\n",
        "    bootstrap_servers=bootstrap_servers, \n",
        "    retries=5,\n",
        "    client_id=f\"{topic_name}-producer-1\",\n",
        "    acks=\"all\"\n",
        ")"
      ]
    },
    {
      "source": [
        "- **client.id** A logical identifier for the client and the application it is used in. This can be any string, and will be used by the brokers to identify messages sent from the   client. It is used in logging and metrics, and for quotas. Choosing a good client name will make troubleshooting much easier - it is the difference between “We are seeing high rate of authentication failures from IP 104.27.155.134” and “Looks like the Order Validation service is failing to authenticate, can you ask Laura to take a look?”\n",
        "\n",
        "\n",
        "- **acks** parameter controls how many partition replicas must receive the record before the producer can consider the write successful. \n",
        "    - **If acks=0**, the producer will not wait for a reply from the broker before assuming the message was sent successfully.\n",
        "    - **If acks=1**, the producer will receive a success response from the broker the moment the leader replica received the message.\n",
        "    - **If acks='all'**, the producer will receive a success response from the broker once all in-sync replicas received the message."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "### Message Delivery Time\n",
        "\n",
        "The producer has multiple configuration parameters that interact to control one of the behaviors that are of most interest to developers: How long will it take until a call to send() will succeed or fail. This is the time we are willing to spend until Kafka responds successfully, or until we are willing to give up and admit defeat.\n",
        "\n",
        "Since Apache Kafka 2.1, we divide the time spent sending a ProduceRecord into two time intervals that are handled separately:\n",
        "\n",
        "- Time until an async call to send() returns - during this interval the thread that called send() will be blocked.\n",
        "\n",
        "- From the time an async call to send() returned successfully until the callback is triggered (with success or failure). This is also from the point a ProduceRecord was placed in a batch for sending, until Kafka responds with success, non-retriable failure, or we run out of time allocated for sending.\n",
        "\n",
        "<img src=\"https://learning.oreilly.com/library/view/kafka-the-definitive/9781492043072/assets/newtimeout.png\" alt=\"producer\" width=\"800\"/>\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "---\n",
        "\n",
        "## Serializers\n",
        "\n",
        "Kafka accept `bytes` as value of `key` and `value`"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "producer.send(topic_name, key=b'key byte', value=b'message byte')\n",
        "producer.flush()"
      ]
    },
    {
      "source": [
        "you could change it to other format by using custom serializers\n",
        "\n",
        "for example: **string serializer**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "producer = KafkaProducer(\n",
        "    bootstrap_servers=bootstrap_servers,\n",
        "    value_serializer=str.encode,\n",
        "    key_serializer=str.encode\n",
        ")\n",
        "\n",
        "producer.send(f\"{topic_name}_str\", key='key_str', value='value str')"
      ]
    },
    {
      "source": [
        "for example: **json serializer**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kafka import KafkaProducer\n",
        "import json\n",
        "\n",
        "producer = KafkaProducer(\n",
        " bootstrap_servers=bootstrap_servers,\n",
        " value_serializer=lambda v: json.dumps(v).encode('ascii'),\n",
        " key_serializer=lambda v: json.dumps(v).encode('ascii')\n",
        ")\n",
        "\n",
        "producer.send(\n",
        "    f\"{topic_name}_json\",\n",
        "    key={\"id\":1},\n",
        "    value={\"name\":\"👨 Francesco\", \"pizza\":\"Margherita 🍕\"}\n",
        ")\n",
        "\n",
        "producer.send(\n",
        "    f\"{topic_name}_json\",\n",
        "    key={\"id\":2},\n",
        "    value={\"name\":\"👩 Adele\", \"pizza\":\"Hawaii 🍕+🍍+🥓\"}\n",
        ")\n",
        "\n",
        "producer.send(\n",
        "    f\"{topic_name}_json\",\n",
        "    key={\"id\":3},\n",
        "    value={\"name\":\"👦 Mark\", \"pizza\":\"Choccolate 🍕+🍫\"}\n",
        ")\n",
        "\n",
        "producer.flush()"
      ]
    },
    {
      "source": [
        "---\n",
        "\n",
        "## If possible use confluent-kafka-python library ([or other specific lang](https://docs.confluent.io/platform/current/clients/index.html))\n",
        "\n",
        "Install libs, [documentation](https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install fastavro\n",
        "!pip install pyrsistent\n",
        "!pip install jsonschema\n",
        "!pip install protobuf\n",
        "!pip install requests\n",
        "!pip install pycodestyle\n",
        "!pip install \"avro-python3==1.9.2\"\n",
        "!pip install confluent-kafka==1.7.0"
      ]
    },
    {
      "source": [
        "Initialize producers\n",
        "\n",
        "all configurations could be seen in here https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from confluent_kafka import Producer\n",
        "\n",
        "producer = Producer({\n",
        "    \"bootstrap.servers\": bootstrap_servers,\n",
        "    \"client.id\": f\"{username}-training-1\"\n",
        "})\n",
        "\n",
        "topic_name = f\"{username}-topic2\""
      ]
    },
    {
      "source": [
        "Asynchronous writes by default"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "producer.produce(topic_name, key=\"key\", value=\"value\")"
      ]
    },
    {
      "source": [
        "handle exceptions"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "producer = Producer({\n",
        "    \"bootstrap.servers\": bootstrap_servers,\n",
        "    \"client.id\": f\"{username}-training-1\"\n",
        "})\n",
        "\n",
        "def acked(err, msg):\n",
        "    if err is not None:\n",
        "        print(\"Failed to deliver message: %s: %s\" % (str(msg), str(err)))\n",
        "    else:\n",
        "        print(\"Message produced: %s\" % (str(msg)))\n",
        "\n",
        "producer.produce(topic_name, key=\"key\", value=\"value\", callback=acked)\n",
        "\n",
        "# Wait up to 1 second for events. Callbacks will be invoked during\n",
        "# this method call if the message is acknowledged.\n",
        "producer.poll(1)"
      ]
    },
    {
      "source": [
        "synchronous write"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "producer.produce(topic_name, key=\"key sync\", value=\"value sync\")\n",
        "producer.flush()"
      ]
    },
    {
      "source": [
        "Typically, `flush()` should be called prior to shutting down the producer to ensure all outstanding/queued/in-flight messages are delivered."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "### Serializer\n",
        "\n",
        "#### JSON"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from confluent_kafka import SerializingProducer\n",
        "from confluent_kafka.serialization import StringSerializer\n",
        "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
        "from confluent_kafka.schema_registry.json_schema import JSONSerializer\n",
        "\n",
        "topic_name_json = f\"{topic_name}_json\"\n",
        "\n",
        "schema_str = \"\"\"\n",
        "{\n",
        "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
        "  \"title\": \"User\",\n",
        "  \"description\": \"A Confluent Kafka Python User\",\n",
        "  \"type\": \"object\",\n",
        "  \"properties\": {\n",
        "    \"name\": {\n",
        "      \"description\": \"User's name\",\n",
        "      \"type\": \"string\"\n",
        "    },\n",
        "    \"favorite_number\": {\n",
        "      \"description\": \"User's favorite number\",\n",
        "      \"type\": \"number\",\n",
        "      \"exclusiveMinimum\": 0\n",
        "    },\n",
        "    \"favorite_color\": {\n",
        "      \"description\": \"User's favorite color\",\n",
        "      \"type\": \"string\"\n",
        "    }\n",
        "  },\n",
        "  \"required\": [ \"name\", \"favorite_number\", \"favorite_color\" ]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def delivery_report(err, msg):\n",
        "    if err is not None:\n",
        "        print(\"Delivery failed for User record {}: {}\".format(msg.key(), err))\n",
        "        return\n",
        "    print('User record {} successfully produced to {} [{}] at offset {}'.format(\n",
        "        msg.key(), msg.topic(), msg.partition(), msg.offset()))\n",
        "\n",
        "\n",
        "schema_registry_conf = {'url': schema_registry_url}\n",
        "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
        "\n",
        "json_serializer = JSONSerializer(schema_str, schema_registry_client)\n",
        "\n",
        "producer_conf = {'bootstrap.servers': bootstrap_servers,\n",
        "                    'key.serializer': StringSerializer('utf_8'),\n",
        "                    'value.serializer': json_serializer}\n",
        "\n",
        "producer = SerializingProducer(producer_conf)\n",
        "\n",
        "user2 = dict(name=\"test\",\n",
        "            favorite_color=\"red\",\n",
        "            favorite_number=1)\n",
        "\n",
        "producer.produce(topic=topic_name_json, key=\"json msg\", value=user2, on_delivery=delivery_report)\n",
        "\n",
        "# flush message for demo\n",
        "producer.flush()"
      ]
    },
    {
      "source": [
        "#### Avro"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from confluent_kafka import avro\n",
        "from confluent_kafka.avro import AvroProducer\n",
        "\n",
        "topic_name_avro = f\"{topic_name}_avro\"\n",
        "\n",
        "value_schema_str = \"\"\"\n",
        "{\n",
        "   \"namespace\": \"my.test\",\n",
        "   \"name\": \"value\",\n",
        "   \"type\": \"record\",\n",
        "   \"fields\" : [\n",
        "     {\n",
        "       \"name\" : \"name\",\n",
        "       \"type\" : \"string\"\n",
        "     }\n",
        "   ]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "key_schema_str = \"\"\"\n",
        "{\n",
        "   \"namespace\": \"my.test\",\n",
        "   \"name\": \"key\",\n",
        "   \"type\": \"record\",\n",
        "   \"fields\" : [\n",
        "     {\n",
        "       \"name\" : \"name\",\n",
        "       \"type\" : \"string\"\n",
        "     }\n",
        "   ]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "value_schema = avro.loads(value_schema_str)\n",
        "key_schema = avro.loads(key_schema_str)\n",
        "value = {\"name\": \"Value\"}\n",
        "key = {\"name\": \"Key\"}\n",
        "\n",
        "def delivery_report(err, msg):\n",
        "    \"\"\" Called once for each message produced to indicate delivery result.\n",
        "        Triggered by poll() or flush(). \"\"\"\n",
        "    if err is not None:\n",
        "        print('Message delivery failed: {}'.format(err))\n",
        "    else:\n",
        "        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))\n",
        "\n",
        "\n",
        "avroProducer = AvroProducer({\n",
        "    'bootstrap.servers': bootstrap_servers,\n",
        "    'on_delivery': delivery_report,\n",
        "    'schema.registry.url': schema_registry_url\n",
        "}, default_key_schema=key_schema, default_value_schema=value_schema)\n",
        "\n",
        "avroProducer.produce(topic=topic_name_avro, value=value, key=key)\n",
        "avroProducer.flush()"
      ]
    },
    {
      "source": [
        "## Confluent schema registry\n",
        "\n",
        "Confluent Schema Registry provides a serving layer for your metadata. It provides a RESTful interface for storing and retrieving your Avro®, JSON Schema, and Protobuf schemas. It stores a versioned history of all schemas based on a specified subject name strategy, provides multiple compatibility settings and allows evolution of schemas according to the configured compatibility settings and expanded support for these schema types. It provides serializers that plug into Apache Kafka® clients that handle schema storage and retrieval for Kafka messages that are sent in any of the supported formats.\n",
        "\n",
        "<img src=\"https://learning.oreilly.com/library/view/kafka-the-definitive/9781492043072/assets/ktdg_0302.png\" alt=\"producer\" width=\"800\"/>\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "---\n",
        "\n",
        "## Partition\n",
        "\n",
        "### Create topic"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from confluent_kafka.admin import AdminClient, NewTopic\n",
        "\n",
        "\n",
        "admin = AdminClient({'bootstrap.servers': bootstrap_servers})\n",
        "\n",
        "topic_name_partitioned = f\"{topic_name}_partitioned\"\n",
        "\n",
        "new_topics = [NewTopic(topic_name_partitioned, num_partitions=3, replication_factor=1)]\n",
        "# Call create_topics to asynchronously create topics, a dict\n",
        "# of <topic,future> is returned.\n",
        "fs = admin.create_topics(new_topics)\n",
        "\n",
        "# Wait for operation to finish.\n",
        "# Timeouts are preferably controlled by passing request_timeout=15.0\n",
        "# to the create_topics() call.\n",
        "# All futures will finish at the same time.\n",
        "for topic, f in fs.items():\n",
        "    try:\n",
        "        f.result()  # The result itself is None\n",
        "        print(\"Topic {} created\".format(topic))\n",
        "    except Exception as e:\n",
        "        print(\"Failed to create topic {}: {}\".format(topic, e))"
      ]
    },
    {
      "source": [
        "### Produce message"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "producer.produce(topic_name_partitioned, key=\"key1\", value=\"value1\")\n",
        "producer.produce(topic_name_partitioned, key=\"key2\", value=\"value2\")\n",
        "producer.produce(topic_name_partitioned, key=\"key3\", value=\"value3\")\n",
        "producer.produce(topic_name_partitioned, key=\"key4\", value=\"value4\")\n",
        "producer.produce(topic_name_partitioned, key=\"key100\", value=\"value4\")\n",
        "producer.produce(topic_name_partitioned, key=\"key101\", value=\"value4\")\n",
        "producer.produce(topic_name_partitioned, key=\"key102\", value=\"value4\")\n",
        "producer.produce(topic_name_partitioned, key=\"key101\", value=\"value4\")\n",
        "producer.produce(topic_name_partitioned, key=\"key101\", value=\"value4\")\n",
        "producer.produce(topic_name_partitioned, key=\"key101\", value=\"value4\")\n",
        "producer.flush()"
      ]
    },
    {
      "source": [
        "by default partitioned by hash\n",
        "\n",
        "but it can produce to specific partition too"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "producer.produce(topic_name_partitioned, key=\"key1\", value=\"value1\", partition=2)\n",
        "producer.produce(topic_name_partitioned, key=\"key2\", value=\"value2\", partition=2)\n",
        "producer.produce(topic_name_partitioned, key=\"key3\", value=\"value3\", partition=2)\n",
        "producer.produce(topic_name_partitioned, key=\"key4\", value=\"value4\", partition=2)\n",
        "producer.flush()"
      ]
    },
    {
      "source": [
        "but you can also create custom partitioner logic on your own, use librdkafka config in here\n",
        "https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md\n",
        "\n",
        "So far, we have discussed the traits of the default partitioner, which is the one most commonly used. However, Kafka does not limit you to just hash partitions, and sometimes there are good reasons to partition data differently. For example, suppose that you are a B2B vendor and your biggest customer is a company that manufactures handheld devices called Bananas. Suppose that you do so much business with customer “Banana” that over 10% of your daily transactions are with this customer. If you use default hash partitioning, the Banana records will get allocated to the same partition as other accounts, resulting in one partition being much larger than the rest. This can cause servers to run out of space, processing to slow down, etc. What we really want is to give Banana its own partition and then use hash partitioning to map the rest of the accounts to partitions.\n",
        "\n",
        "example:\n",
        "```java\n",
        "import org.apache.kafka.clients.producer.Partitioner;\n",
        "import org.apache.kafka.common.Cluster;\n",
        "import org.apache.kafka.common.PartitionInfo;\n",
        "import org.apache.kafka.common.record.InvalidRecordException;\n",
        "import org.apache.kafka.common.utils.Utils;\n",
        "\n",
        "public class BananaPartitioner implements Partitioner {\n",
        "\n",
        "    public void configure(Map<String, ?> configs) {} 1\n",
        "\n",
        "    public int partition(String topic, Object key, byte[] keyBytes,\n",
        "                         Object value, byte[] valueBytes,\n",
        "                         Cluster cluster) {\n",
        "        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);\n",
        "        int numPartitions = partitions.size();\n",
        "\n",
        "        if ((keyBytes == null) || (!(key instanceOf String))) 2\n",
        "            throw new InvalidRecordException(\"We expect all messages \" +\n",
        "                \"to have customer name as key\")\n",
        "\n",
        "        if (((String) key).equals(\"Banana\"))\n",
        "            return numPartitions - 1; // Banana will always go to last partition\n",
        "\n",
        "        // Other records will get hashed to the rest of the partitions\n",
        "        return (Math.abs(Utils.murmur2(keyBytes)) % (numPartitions - 1))\n",
        "    }\n",
        "\n",
        "    public void close() {}\n",
        "}\n",
        "\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}