{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python375jvsc74a57bd07d6993cb2f9ce9a59d5d7380609d9cb5192a9dedd2735a011418ad9e827eb538",
      "display_name": "Python 3.7.5 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "02 - Consumer.ipynb",
      "provenance": []
    },
    "metadata": {
      "interpreter": {
        "hash": "7d6993cb2f9ce9a59d5d7380609d9cb5192a9dedd2735a011418ad9e827eb538"
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RkboZCWrgWp"
      },
      "source": [
        "# Kafka Consumer\n",
        "---\n"
      ],
      "id": "7RkboZCWrgWp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bQ5jxNgrgW2"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "The first step is to install the required libraries, in our case `kafka-python` (and `pandas` to support visualization)"
      ],
      "id": "0bQ5jxNgrgW2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install kafka-python"
      ]
    },
    {
      "source": [
        "declare variable"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "username = \"pujo\"\n",
        "server_ip = \"34.87.150.250\"\n",
        "bootstrap_servers = f\"{server_ip}:9092,{server_ip}:9093,{server_ip}:9094\"\n",
        "schema_registry_url = \"http://34.87.150.250:8081\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzEMppcNrgW3"
      },
      "source": [
        "---\n",
        "\n",
        "## Subscribe from topic\n"
      ],
      "id": "vzEMppcNrgW3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kafka import KafkaConsumer\n",
        "\n",
        "topic_name = f\"{username}-topic1\"\n",
        "\n",
        "consumer = KafkaConsumer(\n",
        "    bootstrap_servers = bootstrap_servers,\n",
        "    max_poll_records = 10\n",
        ")"
      ]
    },
    {
      "source": [
        "check out topics"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "consumer.topics()"
      ]
    },
    {
      "source": [
        "and subscribe to the topic"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "consumer.subscribe(topics=[topic_name])\n",
        "consumer.subscription()"
      ]
    },
    {
      "source": [
        "Now we start reading"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for message in consumer:\n",
        "    print (\"%d:%d: k=%s v=%s\" % (message.partition,\n",
        "                                 message.offset,\n",
        "                                 message.key,\n",
        "                                 message.value))"
      ]
    },
    {
      "source": [
        "join a consumer group for dynamic partition assignment and offset commits"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "group_id = f\"{username}-group1\"\n",
        "client_id = f\"{username}-client1\"\n",
        "\n",
        "consumer = KafkaConsumer(\n",
        "    client_id = client_id,\n",
        "    group_id = group_id,\n",
        "    bootstrap_servers = bootstrap_servers,\n",
        "    max_poll_records = 10\n",
        ")\n",
        "\n",
        "consumer.subscribe(topics=[topic_name])\n",
        "consumer.subscription()\n",
        "\n",
        "for message in consumer:\n",
        "    print (\"%d:%d: k=%s v=%s\" % (message.partition,\n",
        "                                 message.offset,\n",
        "                                 message.key,\n",
        "                                 message.value))"
      ]
    },
    {
      "source": [
        "---\n",
        "\n",
        "## Configuration\n",
        "\n",
        "Kafka consumer API https://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "**auto.offset.reset**\n",
        "\n",
        "This property controls the behavior of the consumer when it starts reading a partition for which it doesn’t have a committed offset or if the committed offset it has is invalid (usually because the consumer was down for so long that the record with that offset was already aged out of the broker). The default is “latest,” which means that lacking a valid offset, the consumer will start reading from the newest records (records that were written after the consumer started running). The alternative is “earliest”\n",
        "\n",
        "for example:\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "group_id = f\"{username}-group1\"\n",
        "client_id = f\"{username}-client1\"\n",
        "\n",
        "consumer = KafkaConsumer(\n",
        "    client_id = client_id,\n",
        "    group_id = group_id + \"_2\",\n",
        "    bootstrap_servers = bootstrap_servers,\n",
        "    max_poll_records = 10,\n",
        "    auto_offset_reset = \"earliest\"\n",
        ")\n",
        "\n",
        "consumer.subscribe(topics=[topic_name])\n",
        "consumer.subscription()\n",
        "\n",
        "for message in consumer:\n",
        "    print (\"%d:%d: k=%s v=%s\" % (message.partition,\n",
        "                                 message.offset,\n",
        "                                 message.key,\n",
        "                                 message.value))"
      ]
    },
    {
      "source": [
        "**enable.auto.commit**\n",
        "\n",
        "This parameter controls whether the consumer will commit offsets automatically, and defaults to true. Set it to false if you prefer to control when offsets are committed, which is necessary to minimize duplicates and avoid missing data. \n",
        "\n",
        "**client.id**\n",
        "\n",
        "This can be any string, and will be used by the brokers to identify messages sent from the client. It is used in logging and metrics, and for quotas.\n",
        "\n",
        "**group.id**\n",
        "\n",
        "This can be any string and is used to provide a consumer with static group membership."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "---\n",
        "\n",
        "## Deserializer"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "### JSON deserializer"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kafka import KafkaConsumer\n",
        "import json\n",
        "\n",
        "group_id = f\"{username}-group1-json\"\n",
        "client_id = f\"{username}-client1\"\n",
        "\n",
        "consumer = KafkaConsumer(\n",
        " client_id = client_id,\n",
        " group_id = group_id,\n",
        " bootstrap_servers = bootstrap_servers,\n",
        " value_deserializer = lambda v: json.loads(v.decode('ascii')),\n",
        " key_deserializer = lambda v: json.loads(v.decode('ascii')),\n",
        " max_poll_records = 10\n",
        ")\n",
        "\n",
        "consumer.subscribe(topics=[f\"{topic_name}_json\"])\n",
        "consumer.subscription()\n",
        "\n",
        "for message in consumer:\n",
        "    print (\"%d:%d: k=%s v=%s\" % (message.partition,\n",
        "                                 message.offset,\n",
        "                                 message.key,\n",
        "                                 message.value))\n"
      ]
    },
    {
      "source": [
        "---\n",
        "\n",
        "## If possible use confluent-kafka-python library ([or other specific lang](https://docs.confluent.io/platform/current/clients/index.html))\n",
        "\n",
        "Install libs, [documentation](https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install fastavro\n",
        "!pip install pyrsistent\n",
        "!pip install jsonschema\n",
        "!pip install protobuf\n",
        "!pip install requests\n",
        "!pip install pycodestyle\n",
        "!pip install \"avro-python3==1.9.2\"\n",
        "!pip install confluent-kafka==1.7.0"
      ]
    },
    {
      "source": [
        "Initialize consumers\n",
        "\n",
        "all configurations could be seen in here https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from confluent_kafka import Consumer\n",
        "\n",
        "group_id = f\"{username}-group2\"\n",
        "client_id = f\"{username}-client2\"\n",
        "\n",
        "\n",
        "consumer = Consumer({\n",
        "    'bootstrap.servers': bootstrap_servers,\n",
        "    'group.id': group_id,\n",
        "    'client.id': client_id,\n",
        "    'auto.offset.reset': 'earliest'\n",
        "})\n",
        "\n",
        "topic_name = f\"{username}-topic2\"\n",
        "\n",
        "consumer.subscribe([topic_name])\n",
        "\n",
        "while True:\n",
        "    msg = consumer.poll(1.0)\n",
        "\n",
        "    if msg is None:\n",
        "        continue\n",
        "    if msg.error():\n",
        "        print(\"Consumer error: {}\".format(msg.error()))\n",
        "        continue\n",
        "\n",
        "    print('Received message: {}'.format(msg.value().decode('utf-8')))\n",
        "\n",
        "consumer.close()"
      ]
    },
    {
      "source": [
        "### Deserializer\n",
        "\n",
        "#### JSON"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from confluent_kafka import DeserializingConsumer\n",
        "from confluent_kafka.schema_registry.json_schema import JSONDeserializer\n",
        "from confluent_kafka.serialization import StringDeserializer\n",
        "\n",
        "topic_name_json = f\"{topic_name}_json\"\n",
        "\n",
        "schema_str = \"\"\"\n",
        "{\n",
        "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
        "    \"title\": \"User\",\n",
        "    \"description\": \"A Confluent Kafka Python User\",\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "    \"name\": {\n",
        "        \"description\": \"User's name\",\n",
        "        \"type\": \"string\"\n",
        "    },\n",
        "    \"favorite_number\": {\n",
        "        \"description\": \"User's favorite number\",\n",
        "        \"type\": \"number\",\n",
        "        \"exclusiveMinimum\": 0\n",
        "    },\n",
        "    \"favorite_color\": {\n",
        "        \"description\": \"User's favorite color\",\n",
        "        \"type\": \"string\"\n",
        "    }\n",
        "    },\n",
        "    \"required\": [ \"name\", \"favorite_number\", \"favorite_color\" ]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "json_deserializer = JSONDeserializer(schema_str)\n",
        "string_deserializer = StringDeserializer('utf_8')\n",
        "\n",
        "consumer_conf = {'bootstrap.servers': bootstrap_servers,\n",
        "                    'key.deserializer': string_deserializer,\n",
        "                    'value.deserializer': json_deserializer,\n",
        "                    'group.id': group_id + '-json',\n",
        "                    'auto.offset.reset': \"earliest\"}\n",
        "\n",
        "consumer = DeserializingConsumer(consumer_conf)\n",
        "consumer.subscribe([topic_name_json])\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        # SIGINT can't be handled when polling, limit timeout to 1 second.\n",
        "        msg = consumer.poll(1.0)\n",
        "        if msg is None:\n",
        "            continue\n",
        "\n",
        "        user = msg.value()\n",
        "        if user is not None:\n",
        "            print(user)\n",
        "    except KeyboardInterrupt:\n",
        "        break\n",
        "\n",
        "consumer.close()"
      ]
    },
    {
      "source": [
        "#### Avro"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from confluent_kafka.avro import AvroConsumer\n",
        "from confluent_kafka.avro.serializer import SerializerError\n",
        "\n",
        "topic_name_avro = f\"{username}-topic2_avro\"\n",
        "group_id = f\"{username}-group2-avro\"\n",
        "client_id = f\"{username}-client2-avro\"\n",
        "\n",
        "consumer = AvroConsumer({\n",
        "    'bootstrap.servers': bootstrap_servers,\n",
        "    'group.id': group_id,\n",
        "    'client.id': client_id,\n",
        "    'auto.offset.reset': 'earliest',\n",
        "    'schema.registry.url': schema_registry_url\n",
        "})\n",
        "\n",
        "consumer.subscribe([topic_name_avro])\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        msg = consumer.poll(1)\n",
        "\n",
        "    except SerializerError as e:\n",
        "        print(\"Message deserialization failed for {}: {}\".format(msg, e))\n",
        "        break\n",
        "\n",
        "    if msg is None:\n",
        "        continue\n",
        "\n",
        "    if msg.error():\n",
        "        print(\"AvroConsumer error: {}\".format(msg.error()))\n",
        "        continue\n",
        "\n",
        "    print(msg.value())\n",
        "\n",
        "consumer.close()"
      ]
    },
    {
      "source": [
        "---\n",
        "\n",
        "## Partition\n",
        "\n",
        "different consumer group has different offset from partition\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from confluent_kafka import Consumer\n",
        "\n",
        "group_id = f\"{username}-group2-a\"\n",
        "client_id = f\"{username}-client2-a\"\n",
        "\n",
        "\n",
        "consumer = Consumer({\n",
        "    'bootstrap.servers': bootstrap_servers,\n",
        "    'group.id': group_id,\n",
        "    'client.id': client_id,\n",
        "    'auto.offset.reset': 'earliest'\n",
        "})\n",
        "\n",
        "topic_name = f\"{username}-topic2\"\n",
        "\n",
        "consumer.subscribe([topic_name])\n",
        "\n",
        "while True:\n",
        "    msg = consumer.poll(1.0)\n",
        "\n",
        "    if msg is None:\n",
        "        continue\n",
        "    if msg.error():\n",
        "        print(\"Consumer error: {}\".format(msg.error()))\n",
        "        continue\n",
        "\n",
        "    print('Received message: {}'.format(msg.value().decode('utf-8')))\n",
        "\n",
        "consumer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from confluent_kafka import Consumer\n",
        "\n",
        "group_id = f\"{username}-group2-b\"\n",
        "client_id = f\"{username}-client2-b\"\n",
        "\n",
        "\n",
        "consumer = Consumer({\n",
        "    'bootstrap.servers': bootstrap_servers,\n",
        "    'group.id': group_id,\n",
        "    'client.id': client_id,\n",
        "    'auto.offset.reset': 'earliest'\n",
        "})\n",
        "\n",
        "topic_name = f\"{username}-topic2\"\n",
        "\n",
        "consumer.subscribe([topic_name])\n",
        "\n",
        "while True:\n",
        "    msg = consumer.poll(1.0)\n",
        "\n",
        "    if msg is None:\n",
        "        continue\n",
        "    if msg.error():\n",
        "        print(\"Consumer error: {}\".format(msg.error()))\n",
        "        continue\n",
        "\n",
        "    print('Received message: {}'.format(msg.value().decode('utf-8')))\n",
        "\n",
        "consumer.close()"
      ]
    }
  ]
}